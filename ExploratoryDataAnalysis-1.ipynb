{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7f6cf1ae-cfa3-4daa-83ab-bb92f4183f56",
   "metadata": {},
   "source": [
    "ANS:1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9455a2fe-36a6-42bd-9579-4c6ad611b685",
   "metadata": {},
   "source": [
    "The wine quality dataset typically includes a variety of features that can be used to predict the quality of the wine. Some of the key features often found in such datasets are:\n",
    "\n",
    "1. Fixed acidity: This feature represents the non-volatile acids in the wine. It can affect the perceived sourness or tartness of the wine, and it is crucial in determining the overall taste and balance of the wine.\n",
    "\n",
    "2. Volatile acidity: This feature refers to the amount of acetic acid in the wine, which can contribute to a vinegary taste. High levels of volatile acidity can negatively impact the wine's quality and taste.\n",
    "\n",
    "3. Citric acid: Citric acid is usually found in small quantities in wines. It can add a refreshing, citrusy flavor and is important for providing a crisp and fresh taste to the wine.\n",
    "\n",
    "4. Residual sugar: This feature represents the amount of sugar remaining in the wine after the fermentation process is complete. It can affect the wine's sweetness and balance, and it is crucial in determining the perceived sweetness or dryness of the wine.\n",
    "\n",
    "5. Chlorides: Chlorides, often derived from salt, can affect the wine's taste and balance. High chloride levels can contribute to a salty or briny taste, which might not be desirable in some wine varieties.\n",
    "\n",
    "6. Free sulfur dioxide and total sulfur dioxide: These features represent the amount of sulfur dioxide present in the wine. Sulfur dioxide is used as a preservative and antioxidant in winemaking. It helps prevent the wine from spoiling and protects its flavor and freshness.\n",
    "\n",
    "7. Density: The density of the wine is an important physical property that can indicate the wine's texture, body, and overall mouthfeel. It can also be indicative of the wine's alcohol content and residual sugar levels.\n",
    "\n",
    "8. pH: The pH level is an essential factor in determining the wine's acidity or basicity. It can influence the wine's taste, stability, and overall balance.\n",
    "\n",
    "9. Sulphates: Sulphates, also known as sulfites, are additives used in winemaking to prevent microbial spoilage and oxidation. They can affect the wine's aroma and flavor, and their presence needs to be carefully controlled to maintain wine quality.\n",
    "\n",
    "10. Alcohol: The alcohol content in wine is a crucial factor that affects its body, flavor, and overall character. It contributes to the wine's taste, texture, and aroma, and it is an essential aspect of wine quality.\n",
    "\n",
    "Understanding and analyzing these features in the wine quality dataset can provide valuable insights into the chemical composition and characteristics of the wine, helping to predict and evaluate its overall quality, taste, and consumer preference. By considering these features, winemakers and researchers can make informed decisions during the winemaking process to produce high-quality wines that meet consumer expectations and preferences."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "247ccb54-7e0e-482e-aa00-cd28c8fb9ae1",
   "metadata": {},
   "source": [
    "ANS:-2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfbd8c7c-b442-421c-9ab8-5ec295d96544",
   "metadata": {},
   "source": [
    "Handling missing data is a critical step in the feature engineering process, especially when dealing with datasets such as the wine quality dataset. Various imputation techniques can be applied to address missing data. Some of the common techniques include mean/median imputation, mode imputation, and sophisticated imputation methods like k-Nearest Neighbors (KNN) and Multiple Imputation by Chained Equations (MICE). Each technique has its own advantages and disadvantages, which are discussed below:\n",
    "\n",
    "1. Mean/Median/Mode Imputation:\n",
    "   - **Advantages:** These methods are simple to implement and can be effective when the missing data is assumed to be missing completely at random (MCAR). They can help preserve the overall sample size and are computationally efficient.\n",
    "   - **Disadvantages:** Mean, median, or mode imputation may distort the original variable's distribution, leading to biased estimates and underestimation of the standard errors. It does not account for the relationships between features, potentially leading to inaccurate results.\n",
    "\n",
    "2. K-Nearest Neighbors (KNN) Imputation:\n",
    "   - **Advantages:** KNN imputation leverages the relationships between features and uses the similarity between data points to impute missing values. It can handle both continuous and categorical data, and it provides more accurate imputations when compared to mean or median imputation.\n",
    "   - **Disadvantages:** KNN imputation can be computationally intensive, especially for large datasets. It may not perform well in high-dimensional spaces and can be sensitive to outliers. Additionally, the choice of the value of k can impact the imputation results.\n",
    "\n",
    "3. Multiple Imputation by Chained Equations (MICE):\n",
    "   - **Advantages:** MICE is a flexible imputation method that can handle complex relationships between variables by using the joint distribution of the observed data. It provides more accurate estimates and can preserve the variability of the data more effectively.\n",
    "   - **Disadvantages:** MICE can be computationally intensive and may require more expertise to implement correctly. It also assumes that the data is missing at random (MAR) and may not perform well with data that is missing not at random (MNAR).\n",
    "\n",
    "When choosing an imputation technique, it is essential to consider the nature of the missing data, the underlying data distribution, the relationships between variables, and the potential impact on downstream analyses. Using the appropriate imputation technique can help minimize bias and improve the overall quality and reliability of the data analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f125ae7f-1217-4941-9e20-3f830724e8e4",
   "metadata": {},
   "source": [
    "ANS:-3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "465bbaff-18d5-413f-be05-eaf75508eb12",
   "metadata": {},
   "source": [
    "Several key factors can affect students' performance in exams. Understanding and analyzing these factors is essential for implementing effective interventions and strategies to improve student outcomes. Some of the key factors include:\n",
    "\n",
    "1. Study habits: The amount of time students spend studying and their study techniques can significantly impact their exam performance.\n",
    "\n",
    "2. Classroom environment: The quality of teaching, classroom engagement, and the overall learning environment can influence students' motivation and academic performance.\n",
    "\n",
    "3. Socioeconomic background: Factors such as parental education, income level, and access to resources can play a crucial role in determining students' academic achievement.\n",
    "\n",
    "4. Psychological factors: Students' attitudes, motivation, self-efficacy, and psychological well-being can affect their ability to perform well in exams.\n",
    "\n",
    "5. Health and well-being: Physical health, mental health, and overall well-being can impact students' cognitive functioning and academic performance.\n",
    "\n",
    "To analyze these factors using statistical techniques, you can employ various methods, including:\n",
    "\n",
    "1. Regression analysis: Use regression analysis to understand the relationship between different factors and students' exam performance. This can help identify the relative influence of each factor on the outcome.\n",
    "\n",
    "2. ANOVA (Analysis of Variance): Conduct ANOVA tests to determine whether there are significant differences in exam performance among different groups based on factors such as socioeconomic background or study habits.\n",
    "\n",
    "3. Correlation analysis: Calculate correlation coefficients to assess the strength and direction of the relationship between various factors and students' exam performance. This can help identify which factors are most strongly associated with academic achievement.\n",
    "\n",
    "4. Factor analysis: Perform factor analysis to identify underlying factors or latent variables that contribute to students' exam performance. This can help uncover patterns and relationships among a set of observed variables.\n",
    "\n",
    "5. Descriptive statistics: Use descriptive statistics, such as mean, median, and standard deviation, to summarize and describe the distribution of variables related to students' academic performance.\n",
    "\n",
    "By applying these statistical techniques, you can gain valuable insights into the key factors that influence students' exam performance and develop targeted interventions and strategies to support student success and improve academic outcomes."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0dab7f47-0987-4a50-af0f-2f6e12e70309",
   "metadata": {},
   "source": [
    "ANS:-4"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09e19d0d-b866-4fd4-8e7b-95582b08872d",
   "metadata": {},
   "source": [
    "Feature engineering is a crucial step in the data preprocessing phase, where you extract relevant information from raw data and transform it into a format suitable for machine learning models. In the context of the student performance dataset, the feature engineering process may involve the following steps:\n",
    "\n",
    "1. Data understanding: Gain a comprehensive understanding of the dataset, including the meaning and type of each variable, to determine which features are relevant to predicting student performance.\n",
    "\n",
    "2. Feature selection: Identify the most relevant features that have a significant impact on student performance. Consider factors such as student demographics, socioeconomic background, study habits, and classroom-related variables.\n",
    "\n",
    "3. Handling missing data: Address missing values in the dataset using appropriate imputation techniques or by removing the data points with missing values, depending on the extent of missing data and the nature of the problem.\n",
    "\n",
    "4. Encoding categorical variables: Convert categorical variables into a numerical format that can be easily interpreted by machine learning algorithms. Use techniques like one-hot encoding or label encoding to transform categorical data.\n",
    "\n",
    "5. Feature scaling: Normalize or standardize numerical features to ensure that the data is on a similar scale. This step is essential for models that are sensitive to the scale of the input features, such as regression or neural network models.\n",
    "\n",
    "6. Feature transformation: Create new features by performing mathematical transformations, such as logarithmic or polynomial transformations, to capture complex relationships and improve the model's predictive performance.\n",
    "\n",
    "7. Feature extraction: Extract relevant information from existing features to create new meaningful features. For example, you can extract features related to study habits from variables like study time or study materials used.\n",
    "\n",
    "8. Dimensionality reduction: Apply dimensionality reduction techniques, such as principal component analysis (PCA) or t-distributed stochastic neighbor embedding (t-SNE), to reduce the number of features while preserving the most important information in the data.\n",
    "\n",
    "Throughout the feature engineering process, it is essential to consider the domain knowledge, explore the data distribution, and continuously assess the impact of feature transformations on the predictive performance of the model. Regular validation and testing of the model using appropriate evaluation metrics can help determine the effectiveness of the feature engineering techniques and the overall model performance."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9112c06-2948-467c-b56e-8fbb7945c584",
   "metadata": {},
   "source": [
    "ANS:-5 To perform exploratory data analysis (EDA) on the wine quality dataset and identify the distribution of each feature, we first need to load the dataset and then examine the distribution of each feature. We can use various visualization techniques, such as histograms, box plots, and density plots, to assess the normality of the data. Here is an example of how to do this using Python:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "494a0087-172d-4bfb-a467-cb6a56faca90",
   "metadata": {},
   "source": [
    "By visualizing the distribution of each feature using histograms or density plots, you can assess the normality of the data. Features that exhibit non-normality may show skewed distributions or significant deviations from the Gaussian distribution. Common transformations that can be applied to improve normality include:\n",
    "\n",
    "1:Log transformation: Use the natural logarithm or other log transformations to reduce right skewness in the data.\n",
    "2:Square root transformation: Apply the square root function to reduce right skewness and stabilize the variance.\n",
    "3:Box-Cox transformation: Use the Box-Cox transformation to stabilize variance and make the data more closely resemble a normal distribution.\n",
    "4:Yeo-Johnson transformation: Similar to the Box-Cox transformation, the Yeo-Johnson transformation is effective in handling both positive and negative data values.\n",
    "By applying these transformations to the features exhibiting non-normality, you can improve the normality of the data and ensure that the assumptions of the statistical models are met, leading to more reliable and accurate analyses.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adbb3f52-a7d1-4912-a204-2fe10328dc92",
   "metadata": {},
   "source": [
    "ANS:-6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c351718e-c0d6-4594-aef8-3990db0e5f10",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'wine_quality.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [4], line 6\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpreprocessing\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m StandardScaler\n\u001b[1;32m      5\u001b[0m \u001b[38;5;66;03m# Load the wine quality dataset\u001b[39;00m\n\u001b[0;32m----> 6\u001b[0m wine_data \u001b[38;5;241m=\u001b[39m \u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_csv\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mwine_quality.csv\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;66;03m# Separate the features and the target variable\u001b[39;00m\n\u001b[1;32m      9\u001b[0m X \u001b[38;5;241m=\u001b[39m wine_data\u001b[38;5;241m.\u001b[39mdrop(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mquality\u001b[39m\u001b[38;5;124m'\u001b[39m, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/pandas/util/_decorators.py:211\u001b[0m, in \u001b[0;36mdeprecate_kwarg.<locals>._deprecate_kwarg.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    209\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    210\u001b[0m         kwargs[new_arg_name] \u001b[38;5;241m=\u001b[39m new_arg_value\n\u001b[0;32m--> 211\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/pandas/util/_decorators.py:331\u001b[0m, in \u001b[0;36mdeprecate_nonkeyword_arguments.<locals>.decorate.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    325\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(args) \u001b[38;5;241m>\u001b[39m num_allow_args:\n\u001b[1;32m    326\u001b[0m     warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[1;32m    327\u001b[0m         msg\u001b[38;5;241m.\u001b[39mformat(arguments\u001b[38;5;241m=\u001b[39m_format_argument_list(allow_args)),\n\u001b[1;32m    328\u001b[0m         \u001b[38;5;167;01mFutureWarning\u001b[39;00m,\n\u001b[1;32m    329\u001b[0m         stacklevel\u001b[38;5;241m=\u001b[39mfind_stack_level(),\n\u001b[1;32m    330\u001b[0m     )\n\u001b[0;32m--> 331\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/pandas/io/parsers/readers.py:950\u001b[0m, in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, error_bad_lines, warn_bad_lines, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options)\u001b[0m\n\u001b[1;32m    935\u001b[0m kwds_defaults \u001b[38;5;241m=\u001b[39m _refine_defaults_read(\n\u001b[1;32m    936\u001b[0m     dialect,\n\u001b[1;32m    937\u001b[0m     delimiter,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    946\u001b[0m     defaults\u001b[38;5;241m=\u001b[39m{\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdelimiter\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m,\u001b[39m\u001b[38;5;124m\"\u001b[39m},\n\u001b[1;32m    947\u001b[0m )\n\u001b[1;32m    948\u001b[0m kwds\u001b[38;5;241m.\u001b[39mupdate(kwds_defaults)\n\u001b[0;32m--> 950\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_read\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/pandas/io/parsers/readers.py:605\u001b[0m, in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    602\u001b[0m _validate_names(kwds\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnames\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m))\n\u001b[1;32m    604\u001b[0m \u001b[38;5;66;03m# Create the parser.\u001b[39;00m\n\u001b[0;32m--> 605\u001b[0m parser \u001b[38;5;241m=\u001b[39m \u001b[43mTextFileReader\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    607\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m chunksize \u001b[38;5;129;01mor\u001b[39;00m iterator:\n\u001b[1;32m    608\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m parser\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/pandas/io/parsers/readers.py:1442\u001b[0m, in \u001b[0;36mTextFileReader.__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m   1439\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m kwds[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m   1441\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles: IOHandles \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m-> 1442\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_engine \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_make_engine\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mengine\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/pandas/io/parsers/readers.py:1735\u001b[0m, in \u001b[0;36mTextFileReader._make_engine\u001b[0;34m(self, f, engine)\u001b[0m\n\u001b[1;32m   1733\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m mode:\n\u001b[1;32m   1734\u001b[0m         mode \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m-> 1735\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;241m=\u001b[39m \u001b[43mget_handle\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1736\u001b[0m \u001b[43m    \u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1737\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1738\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mencoding\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1739\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcompression\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcompression\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1740\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmemory_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmemory_map\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1741\u001b[0m \u001b[43m    \u001b[49m\u001b[43mis_text\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_text\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1742\u001b[0m \u001b[43m    \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mencoding_errors\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstrict\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1743\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstorage_options\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1744\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1745\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1746\u001b[0m f \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles\u001b[38;5;241m.\u001b[39mhandle\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/pandas/io/common.py:856\u001b[0m, in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    851\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(handle, \u001b[38;5;28mstr\u001b[39m):\n\u001b[1;32m    852\u001b[0m     \u001b[38;5;66;03m# Check whether the filename is to be opened in binary mode.\u001b[39;00m\n\u001b[1;32m    853\u001b[0m     \u001b[38;5;66;03m# Binary mode does not support 'encoding' and 'newline'.\u001b[39;00m\n\u001b[1;32m    854\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mencoding \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mmode:\n\u001b[1;32m    855\u001b[0m         \u001b[38;5;66;03m# Encoding\u001b[39;00m\n\u001b[0;32m--> 856\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[1;32m    857\u001b[0m \u001b[43m            \u001b[49m\u001b[43mhandle\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    858\u001b[0m \u001b[43m            \u001b[49m\u001b[43mioargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    859\u001b[0m \u001b[43m            \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mioargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencoding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    860\u001b[0m \u001b[43m            \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43merrors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    861\u001b[0m \u001b[43m            \u001b[49m\u001b[43mnewline\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    862\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    863\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    864\u001b[0m         \u001b[38;5;66;03m# Binary mode\u001b[39;00m\n\u001b[1;32m    865\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mopen\u001b[39m(handle, ioargs\u001b[38;5;241m.\u001b[39mmode)\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'wine_quality.csv'"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Load the wine quality dataset\n",
    "wine_data = pd.read_csv('wine_quality.csv')\n",
    "\n",
    "# Separate the features and the target variable\n",
    "X = wine_data.drop('quality', axis=1)\n",
    "\n",
    "# Standardize the features\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "# Perform PCA\n",
    "pca = PCA()\n",
    "X_pca = pca.fit_transform(X_scaled)\n",
    "\n",
    "# Calculate the explained variance ratio\n",
    "explained_variance_ratio = pca.explained_variance_ratio_\n",
    "\n",
    "# Find the minimum number of principal components required to explain 90% of the variance\n",
    "cumulative_variance_ratio = np.cumsum(explained_variance_ratio)\n",
    "num_components = np.argmax(cumulative_variance_ratio >= 0.9) + 1\n",
    "\n",
    "print(f\"Minimum number of principal components to explain 90% of the variance: {num_components}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c6d8b89-94a4-4142-b763-59b4e671a4af",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
